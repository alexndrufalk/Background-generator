{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/B2fXIFd0W2ioeDEEEoVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandrufalk/Background-generator/blob/Layout-page/CenterNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "metadata": {
        "id": "GnYPazoswp_i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Pascal VOC Dataset"
      ],
      "metadata": {
        "id": "Kj0nN5tTcwX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pascal VOC 2007 dataset with bounding boxes\n",
        "dataset, info = tfds.load(\"voc/2007\", split=\"train+validation\", with_info=True)"
      ],
      "metadata": {
        "id": "FHCbdFa7cz1I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Explore the Dataset"
      ],
      "metadata": {
        "id": "j2JA7Vk5dDTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset info\n",
        "print(info)\n",
        "\n",
        "# Example: Iterate through the dataset\n",
        "for sample in dataset.take(1):\n",
        "    image = sample[\"image\"]\n",
        "    bbox = sample[\"objects\"][\"bbox\"]\n",
        "    label = sample[\"objects\"][\"label\"]\n",
        "    print(f\"Image shape: {image.shape}\")\n",
        "    print(f\"Bounding Boxes: {bbox}\")\n",
        "    print(f\"Labels: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DQeUS2jZdEmC",
        "outputId": "6eb567df-2ee1-4b71-d9bd-c3ea7c2e38cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='voc',\n",
            "    full_name='voc/2007/4.0.0',\n",
            "    description=\"\"\"\n",
            "    This dataset contains the data from the PASCAL Visual Object Classes Challenge,\n",
            "    corresponding to the Classification and Detection competitions.\n",
            "    \n",
            "    In the Classification competition, the goal is to predict the set of labels\n",
            "    contained in the image, while in the Detection competition the goal is to\n",
            "    predict the bounding box and label of each individual object.\n",
            "    WARNING: As per the official dataset, the test set of VOC2012 does not contain\n",
            "    annotations.\n",
            "    \"\"\",\n",
            "    config_description=\"\"\"\n",
            "    This dataset contains the data from the PASCAL Visual Object Classes Challenge\n",
            "    2007, a.k.a. VOC2007.\n",
            "    \n",
            "    A total of 9963 images are included in this dataset, where each image\n",
            "    contains a set of objects, out of 20 different classes, making a total of\n",
            "    24640 annotated objects.\n",
            "    \n",
            "    \"\"\",\n",
            "    homepage='http://host.robots.ox.ac.uk/pascal/VOC/voc2007/',\n",
            "    data_dir='/root/tensorflow_datasets/voc/2007/4.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=868.85 MiB,\n",
            "    dataset_size=837.73 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
            "        'image/filename': Text(shape=(), dtype=string),\n",
            "        'labels': Sequence(ClassLabel(shape=(), dtype=int64, num_classes=20)),\n",
            "        'labels_no_difficult': Sequence(ClassLabel(shape=(), dtype=int64, num_classes=20)),\n",
            "        'objects': Sequence({\n",
            "            'bbox': BBoxFeature(shape=(4,), dtype=float32),\n",
            "            'is_difficult': bool,\n",
            "            'is_truncated': bool,\n",
            "            'label': ClassLabel(shape=(), dtype=int64, num_classes=20),\n",
            "            'pose': ClassLabel(shape=(), dtype=int64, num_classes=5),\n",
            "        }),\n",
            "    }),\n",
            "    supervised_keys=None,\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=4952, num_shards=4>,\n",
            "        'train': <SplitInfo num_examples=2501, num_shards=2>,\n",
            "        'validation': <SplitInfo num_examples=2510, num_shards=2>,\n",
            "    },\n",
            "    citation=\"\"\"@misc{pascal-voc-2007,\n",
            "    \tauthor = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n",
            "    \ttitle = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\",\n",
            "    \thowpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"}\"\"\",\n",
            ")\n",
            "Image shape: (480, 389, 3)\n",
            "Bounding Boxes: [[0.14375    0.0437018  0.97083336 0.7763496 ]\n",
            " [0.14583333 0.24164525 0.57916665 0.6066838 ]\n",
            " [0.6        0.5244216  0.8541667  0.76606685]\n",
            " [0.56041664 0.5012854  0.7395833  0.6863753 ]]\n",
            "Labels: [12 14 12 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess the Dataset"
      ],
      "metadata": {
        "id": "FDxEdNoJ1H7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sample_fixed(sample, max_boxes=100):\n",
        "    image = tf.image.resize(sample[\"image\"], (512, 512)) / 255.0  # Normalize image\n",
        "    boxes = sample[\"objects\"][\"bbox\"]  # [ymin, xmin, ymax, xmax]\n",
        "    classes = sample[\"objects\"][\"label\"]\n",
        "\n",
        "    # Convert to CenterNet format\n",
        "    center_x = (boxes[:, 1] + boxes[:, 3]) / 2.0  # (xmin + xmax) / 2\n",
        "    center_y = (boxes[:, 0] + boxes[:, 2]) / 2.0  # (ymin + ymax) / 2\n",
        "    width = boxes[:, 3] - boxes[:, 1]  # xmax - xmin\n",
        "    height = boxes[:, 2] - boxes[:, 0]  # ymax - ymin\n",
        "    centernet_boxes = tf.stack([center_x, center_y, width, height], axis=-1)\n",
        "\n",
        "    # Pad to fixed size\n",
        "    padded_boxes = tf.pad(centernet_boxes, [[0, max_boxes - tf.shape(centernet_boxes)[0]], [0, 0]])\n",
        "    padded_classes = tf.pad(classes, [[0, max_boxes - tf.shape(classes)[0]]])\n",
        "\n",
        "    return image, {\"boxes\": padded_boxes, \"classes\": padded_classes}\n",
        "\n",
        "train_dataset = dataset.map(lambda x: preprocess_sample_fixed(x)).batch(32).shuffle(1000)"
      ],
      "metadata": {
        "id": "PATC0I5P1JZG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Heatmaps"
      ],
      "metadata": {
        "id": "m0SCxExiQzSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_heatmap(boxes, labels, output_shape=(128, 128), num_classes=20, sigma=2):\n",
        "    heatmap = tf.zeros([output_shape[0], output_shape[1], num_classes], dtype=tf.float32)\n",
        "\n",
        "    for box, label in zip(boxes, labels):\n",
        "        cx, cy, w, h = box\n",
        "        grid_x, grid_y = int(cx * output_shape[1]), int(cy * output_shape[0])\n",
        "\n",
        "        # Draw Gaussian around the center\n",
        "        for i in range(max(0, grid_x - 2 * sigma), min(output_shape[1], grid_x + 2 * sigma)):\n",
        "            for j in range(max(0, grid_y - 2 * sigma), min(output_shape[0], grid_y + 2 * sigma)):\n",
        "                heatmap = tf.tensor_scatter_nd_update(\n",
        "                    heatmap,\n",
        "                    indices=[[j, i, label]],\n",
        "                    updates=[tf.exp(-((i - grid_x) ** 2 + (j - grid_y) ** 2) / (2 * sigma ** 2))]\n",
        "                )\n",
        "\n",
        "    return heatmap\n"
      ],
      "metadata": {
        "id": "_fTHBJFEQ0t1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, targets in train_dataset:\n",
        "    heatmap = generate_heatmap(targets[\"boxes\"], targets[\"classes\"])\n",
        "    # Combine heatmap, offsets, and sizes into one structure for training\n",
        "    # Pass the data to your model training loop."
      ],
      "metadata": {
        "id": "PZwyuQHVN5nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the Backbone"
      ],
      "metadata": {
        "id": "_c8LxFk1wpTU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPoZ_t4kwHwb"
      },
      "outputs": [],
      "source": [
        "def create_backbone(input_shape=(512, 512, 3)):\n",
        "    # Use ResNet50 as backbone without top layers, outputting feature maps\n",
        "    backbone = ResNet50(input_shape=input_shape, include_top=False, weights=\"imagenet\")\n",
        "    backbone_output = backbone.get_layer(\"conv4_block6_out\").output  # Choose a layer\n",
        "    model = tf.keras.Model(inputs=backbone.input, outputs=backbone_output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Detection Head\n",
        "The detection head will predict:\n",
        "\n",
        "-Heatmap: to identify object centers.\n",
        "\n",
        "-Offsets: to adjust for downsampling effects.\n",
        "\n",
        "-Sizes: to predict the width and height of objects.\n"
      ],
      "metadata": {
        "id": "0wZHjLAyxAZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_detection_head(backbone_output, num_classes):\n",
        "    # Heatmap head\n",
        "    heatmap = tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(backbone_output)\n",
        "    heatmap = tf.keras.layers.Conv2D(num_classes, kernel_size=1, padding=\"same\", activation=\"sigmoid\", name=\"heatmap\")(heatmap)\n",
        "\n",
        "    # Offset head\n",
        "    offset = tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(backbone_output)\n",
        "    offset = tf.keras.layers.Conv2D(2, kernel_size=1, padding=\"same\", name=\"offset\")(offset)  # 2 channels for x and y offsets\n",
        "\n",
        "    # Size head\n",
        "    size = tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(backbone_output)\n",
        "    size = tf.keras.layers.Conv2D(2, kernel_size=1, padding=\"same\", name=\"size\")(size)  # 2 channels for width and height\n",
        "\n",
        "    return heatmap, offset, size\n"
      ],
      "metadata": {
        "id": "Jww7034LxR9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the Complete CenterNet Model"
      ],
      "metadata": {
        "id": "v5UNB90LA66B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_centernet(input_shape=(512, 512, 3), num_classes=80):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    backbone_output = create_backbone(input_shape)(inputs)\n",
        "    heatmap, offset, size = create_detection_head(backbone_output, num_classes)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=[heatmap, offset, size])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "E-_X853SA7tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compile the Model"
      ],
      "metadata": {
        "id": "fre-mqKYBAyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        alpha_factor = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)\n",
        "        focal_weight = tf.where(tf.equal(y_true, 1), 1 - y_pred, y_pred)\n",
        "        focal_weight = alpha_factor * tf.pow(focal_weight, gamma)\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        return focal_weight * bce\n",
        "    return focal_loss_fixed\n",
        "\n",
        "def smooth_l1_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.Huber()(y_true, y_pred)\n",
        "\n",
        "centernet = create_centernet()\n",
        "centernet.compile(optimizer='adam',\n",
        "                  loss={'heatmap': focal_loss(),\n",
        "                        'offset': smooth_l1_loss,\n",
        "                        'size': smooth_l1_loss})"
      ],
      "metadata": {
        "id": "epLB0IsdBBre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Model"
      ],
      "metadata": {
        "id": "LlsztyaLGFxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = centernet.fit(train_dataset, validation_data=val_dataset, epochs=20)"
      ],
      "metadata": {
        "id": "Cp25jdF6GFAN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}